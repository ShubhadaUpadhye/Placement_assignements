{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NLP_answer_2\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import html\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "##reading contents of pdf file###\n",
    "pdf_reader = PyPDF2.PdfReader('civil_lecture_notes.pdf')\n",
    "print(len(pdf_reader.pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##extracting text from pdf_file###\n",
    "pdf_text=[]\n",
    "def extract_text(pdf_file):\n",
    "    for i in range(len(pdf_reader.pages)):\n",
    "        pdf_text.append(pdf_reader.pages[i].extract_text())\n",
    "extract_text(pdf_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent words are: ['of']\n",
      "the length of each word respectively is: [2]\n",
      "of is the most frequent word and 2 is the corresponding highest length\n"
     ]
    }
   ],
   "source": [
    "###getting word that has highest occurance\n",
    "def frequency(text):\n",
    "    length_list=[]\n",
    "    occurance_list=[]\n",
    "    count_list=[]\n",
    "    words_list=[]\n",
    "    lem_words=[]\n",
    "#######removing punctuations and  converting string to lower_case########\n",
    "    try:\n",
    "        text=text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text=text.lower()\n",
    "            \n",
    "####### splitting string and creating list of same########\n",
    "        token_words = nltk.word_tokenize(text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "######lemmatization of words##########\n",
    "    def lemmatize_words(token_words):\n",
    "        lm= WordNetLemmatizer()\n",
    "        for word in token_words:\n",
    "            lem_words.append(lm.lemmatize(word))\n",
    "    lemmatize_words(token_words)\n",
    "############## finding frequency of every word in string_list and appending it to occurance_list###########\n",
    "    try:\n",
    "        for words in set(token_words):\n",
    "            occurances=token_words.count(words)\n",
    "            occurance_list.append(occurances)\n",
    "############## sorting occurance_list is descending_order to get highest occurance###############\n",
    "            occurance_list.sort(reverse=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "################# find all the similar highest occurances and appending it to count_list ###########\n",
    "    try:\n",
    "        for occurances in range(len(occurance_list)):\n",
    "            if occurance_list[0]==occurance_list[occurances]:\n",
    "                count_list.append(occurance_list[occurances])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "####################### finding the words with highest occurances and appending them to words_list ##############    \n",
    "    try: \n",
    "        for counts in count_list:\n",
    "            for words in set(token_words):\n",
    "                if counts==token_words.count(words):\n",
    "                    words_list.append(words)\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    else:\n",
    "        print(\"The most frequent words are:\",words_list)\n",
    "######################### finding length of all highest occuring words ###################\n",
    "    try:\n",
    "        for words in words_list:\n",
    "            length_list.append(len(words))\n",
    "        print(\"the length of each word respectively is:\",length_list)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    else:\n",
    "        length_list.sort(reverse=True)\n",
    "######################### extracting words with highest length and occurances###################\n",
    "    try:\n",
    "        for words in words_list:\n",
    "            if length_list[0]==len(words):\n",
    "                print('{}'.format(words),\"is the most frequent word and\",'{}'.format(length_list[0]),\"is the corresponding highest length\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "for content in pdf_text:\n",
    "    ##removing_html_tags##\n",
    "    soup=bs(content,'html.parser').text\n",
    "    text=html.unescape(soup)\n",
    "    frequency(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
